{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["## Programming Assignment #6\n","\n","**Machine Learning Validation**\n","\n","100 points possible.\n","\n","This assignment asks you to (1) compare validation strategies for a ML model and (2) get experience trying different models."],"metadata":{"id":"vyViyXVm_LpM"}},{"cell_type":"markdown","source":["##The Setting -- Breast Cancer Prediction\n","\n","The code below loads a publicly available data set for breast cancer; the target is a cancer status (benign or malignant) and the features are measurements of the observed mass. The data set description (DESCR) is printed below for additional context."],"metadata":{"id":"KqDnnIfY_nuQ"}},{"cell_type":"code","source":["from sklearn.datasets import load_breast_cancer\n","cancer = load_breast_cancer(as_frame=True)\n","cancer_df = cancer.frame\n","print(cancer.DESCR)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kUMy9zEDkol2","outputId":"bc7e14fd-a4b8-4026-8db2-a9c288858bfc","executionInfo":{"status":"ok","timestamp":1670119181051,"user_tz":360,"elapsed":3198,"user":{"displayName":"Matt Massey","userId":"08637701140305391198"}}},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":[".. _breast_cancer_dataset:\n","\n","Breast cancer wisconsin (diagnostic) dataset\n","--------------------------------------------\n","\n","**Data Set Characteristics:**\n","\n","    :Number of Instances: 569\n","\n","    :Number of Attributes: 30 numeric, predictive attributes and the class\n","\n","    :Attribute Information:\n","        - radius (mean of distances from center to points on the perimeter)\n","        - texture (standard deviation of gray-scale values)\n","        - perimeter\n","        - area\n","        - smoothness (local variation in radius lengths)\n","        - compactness (perimeter^2 / area - 1.0)\n","        - concavity (severity of concave portions of the contour)\n","        - concave points (number of concave portions of the contour)\n","        - symmetry\n","        - fractal dimension (\"coastline approximation\" - 1)\n","\n","        The mean, standard error, and \"worst\" or largest (mean of the three\n","        worst/largest values) of these features were computed for each image,\n","        resulting in 30 features.  For instance, field 0 is Mean Radius, field\n","        10 is Radius SE, field 20 is Worst Radius.\n","\n","        - class:\n","                - WDBC-Malignant\n","                - WDBC-Benign\n","\n","    :Summary Statistics:\n","\n","    ===================================== ====== ======\n","                                           Min    Max\n","    ===================================== ====== ======\n","    radius (mean):                        6.981  28.11\n","    texture (mean):                       9.71   39.28\n","    perimeter (mean):                     43.79  188.5\n","    area (mean):                          143.5  2501.0\n","    smoothness (mean):                    0.053  0.163\n","    compactness (mean):                   0.019  0.345\n","    concavity (mean):                     0.0    0.427\n","    concave points (mean):                0.0    0.201\n","    symmetry (mean):                      0.106  0.304\n","    fractal dimension (mean):             0.05   0.097\n","    radius (standard error):              0.112  2.873\n","    texture (standard error):             0.36   4.885\n","    perimeter (standard error):           0.757  21.98\n","    area (standard error):                6.802  542.2\n","    smoothness (standard error):          0.002  0.031\n","    compactness (standard error):         0.002  0.135\n","    concavity (standard error):           0.0    0.396\n","    concave points (standard error):      0.0    0.053\n","    symmetry (standard error):            0.008  0.079\n","    fractal dimension (standard error):   0.001  0.03\n","    radius (worst):                       7.93   36.04\n","    texture (worst):                      12.02  49.54\n","    perimeter (worst):                    50.41  251.2\n","    area (worst):                         185.2  4254.0\n","    smoothness (worst):                   0.071  0.223\n","    compactness (worst):                  0.027  1.058\n","    concavity (worst):                    0.0    1.252\n","    concave points (worst):               0.0    0.291\n","    symmetry (worst):                     0.156  0.664\n","    fractal dimension (worst):            0.055  0.208\n","    ===================================== ====== ======\n","\n","    :Missing Attribute Values: None\n","\n","    :Class Distribution: 212 - Malignant, 357 - Benign\n","\n","    :Creator:  Dr. William H. Wolberg, W. Nick Street, Olvi L. Mangasarian\n","\n","    :Donor: Nick Street\n","\n","    :Date: November, 1995\n","\n","This is a copy of UCI ML Breast Cancer Wisconsin (Diagnostic) datasets.\n","https://goo.gl/U2Uwz2\n","\n","Features are computed from a digitized image of a fine needle\n","aspirate (FNA) of a breast mass.  They describe\n","characteristics of the cell nuclei present in the image.\n","\n","Separating plane described above was obtained using\n","Multisurface Method-Tree (MSM-T) [K. P. Bennett, \"Decision Tree\n","Construction Via Linear Programming.\" Proceedings of the 4th\n","Midwest Artificial Intelligence and Cognitive Science Society,\n","pp. 97-101, 1992], a classification method which uses linear\n","programming to construct a decision tree.  Relevant features\n","were selected using an exhaustive search in the space of 1-4\n","features and 1-3 separating planes.\n","\n","The actual linear program used to obtain the separating plane\n","in the 3-dimensional space is that described in:\n","[K. P. Bennett and O. L. Mangasarian: \"Robust Linear\n","Programming Discrimination of Two Linearly Inseparable Sets\",\n","Optimization Methods and Software 1, 1992, 23-34].\n","\n","This database is also available through the UW CS ftp server:\n","\n","ftp ftp.cs.wisc.edu\n","cd math-prog/cpo-dataset/machine-learn/WDBC/\n","\n",".. topic:: References\n","\n","   - W.N. Street, W.H. Wolberg and O.L. Mangasarian. Nuclear feature extraction \n","     for breast tumor diagnosis. IS&T/SPIE 1993 International Symposium on \n","     Electronic Imaging: Science and Technology, volume 1905, pages 861-870,\n","     San Jose, CA, 1993.\n","   - O.L. Mangasarian, W.N. Street and W.H. Wolberg. Breast cancer diagnosis and \n","     prognosis via linear programming. Operations Research, 43(4), pages 570-577, \n","     July-August 1995.\n","   - W.H. Wolberg, W.N. Street, and O.L. Mangasarian. Machine learning techniques\n","     to diagnose breast cancer from fine-needle aspirates. Cancer Letters 77 (1994) \n","     163-171.\n"]}]},{"cell_type":"code","source":["# The measurements are numerical values\n","cancer_df.head(n=20)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":805},"id":"l0agOF8Zmet7","outputId":"cdd0781f-4e9f-4053-b44f-325f61bdc504","executionInfo":{"status":"ok","timestamp":1670119188155,"user_tz":360,"elapsed":322,"user":{"displayName":"Matt Massey","userId":"08637701140305391198"}}},"execution_count":2,"outputs":[{"output_type":"execute_result","data":{"text/plain":["    mean radius  mean texture  mean perimeter  mean area  mean smoothness  \\\n","0         17.99         10.38          122.80     1001.0          0.11840   \n","1         20.57         17.77          132.90     1326.0          0.08474   \n","2         19.69         21.25          130.00     1203.0          0.10960   \n","3         11.42         20.38           77.58      386.1          0.14250   \n","4         20.29         14.34          135.10     1297.0          0.10030   \n","5         12.45         15.70           82.57      477.1          0.12780   \n","6         18.25         19.98          119.60     1040.0          0.09463   \n","7         13.71         20.83           90.20      577.9          0.11890   \n","8         13.00         21.82           87.50      519.8          0.12730   \n","9         12.46         24.04           83.97      475.9          0.11860   \n","10        16.02         23.24          102.70      797.8          0.08206   \n","11        15.78         17.89          103.60      781.0          0.09710   \n","12        19.17         24.80          132.40     1123.0          0.09740   \n","13        15.85         23.95          103.70      782.7          0.08401   \n","14        13.73         22.61           93.60      578.3          0.11310   \n","15        14.54         27.54           96.73      658.8          0.11390   \n","16        14.68         20.13           94.74      684.5          0.09867   \n","17        16.13         20.68          108.10      798.8          0.11700   \n","18        19.81         22.15          130.00     1260.0          0.09831   \n","19        13.54         14.36           87.46      566.3          0.09779   \n","\n","    mean compactness  mean concavity  mean concave points  mean symmetry  \\\n","0            0.27760         0.30010              0.14710         0.2419   \n","1            0.07864         0.08690              0.07017         0.1812   \n","2            0.15990         0.19740              0.12790         0.2069   \n","3            0.28390         0.24140              0.10520         0.2597   \n","4            0.13280         0.19800              0.10430         0.1809   \n","5            0.17000         0.15780              0.08089         0.2087   \n","6            0.10900         0.11270              0.07400         0.1794   \n","7            0.16450         0.09366              0.05985         0.2196   \n","8            0.19320         0.18590              0.09353         0.2350   \n","9            0.23960         0.22730              0.08543         0.2030   \n","10           0.06669         0.03299              0.03323         0.1528   \n","11           0.12920         0.09954              0.06606         0.1842   \n","12           0.24580         0.20650              0.11180         0.2397   \n","13           0.10020         0.09938              0.05364         0.1847   \n","14           0.22930         0.21280              0.08025         0.2069   \n","15           0.15950         0.16390              0.07364         0.2303   \n","16           0.07200         0.07395              0.05259         0.1586   \n","17           0.20220         0.17220              0.10280         0.2164   \n","18           0.10270         0.14790              0.09498         0.1582   \n","19           0.08129         0.06664              0.04781         0.1885   \n","\n","    mean fractal dimension  ...  worst texture  worst perimeter  worst area  \\\n","0                  0.07871  ...          17.33           184.60      2019.0   \n","1                  0.05667  ...          23.41           158.80      1956.0   \n","2                  0.05999  ...          25.53           152.50      1709.0   \n","3                  0.09744  ...          26.50            98.87       567.7   \n","4                  0.05883  ...          16.67           152.20      1575.0   \n","5                  0.07613  ...          23.75           103.40       741.6   \n","6                  0.05742  ...          27.66           153.20      1606.0   \n","7                  0.07451  ...          28.14           110.60       897.0   \n","8                  0.07389  ...          30.73           106.20       739.3   \n","9                  0.08243  ...          40.68            97.65       711.4   \n","10                 0.05697  ...          33.88           123.80      1150.0   \n","11                 0.06082  ...          27.28           136.50      1299.0   \n","12                 0.07800  ...          29.94           151.70      1332.0   \n","13                 0.05338  ...          27.66           112.00       876.5   \n","14                 0.07682  ...          32.01           108.80       697.7   \n","15                 0.07077  ...          37.13           124.10       943.2   \n","16                 0.05922  ...          30.88           123.40      1138.0   \n","17                 0.07356  ...          31.48           136.80      1315.0   \n","18                 0.05395  ...          30.88           186.80      2398.0   \n","19                 0.05766  ...          19.26            99.70       711.2   \n","\n","    worst smoothness  worst compactness  worst concavity  \\\n","0             0.1622             0.6656           0.7119   \n","1             0.1238             0.1866           0.2416   \n","2             0.1444             0.4245           0.4504   \n","3             0.2098             0.8663           0.6869   \n","4             0.1374             0.2050           0.4000   \n","5             0.1791             0.5249           0.5355   \n","6             0.1442             0.2576           0.3784   \n","7             0.1654             0.3682           0.2678   \n","8             0.1703             0.5401           0.5390   \n","9             0.1853             1.0580           1.1050   \n","10            0.1181             0.1551           0.1459   \n","11            0.1396             0.5609           0.3965   \n","12            0.1037             0.3903           0.3639   \n","13            0.1131             0.1924           0.2322   \n","14            0.1651             0.7725           0.6943   \n","15            0.1678             0.6577           0.7026   \n","16            0.1464             0.1871           0.2914   \n","17            0.1789             0.4233           0.4784   \n","18            0.1512             0.3150           0.5372   \n","19            0.1440             0.1773           0.2390   \n","\n","    worst concave points  worst symmetry  worst fractal dimension  target  \n","0                0.26540          0.4601                  0.11890       0  \n","1                0.18600          0.2750                  0.08902       0  \n","2                0.24300          0.3613                  0.08758       0  \n","3                0.25750          0.6638                  0.17300       0  \n","4                0.16250          0.2364                  0.07678       0  \n","5                0.17410          0.3985                  0.12440       0  \n","6                0.19320          0.3063                  0.08368       0  \n","7                0.15560          0.3196                  0.11510       0  \n","8                0.20600          0.4378                  0.10720       0  \n","9                0.22100          0.4366                  0.20750       0  \n","10               0.09975          0.2948                  0.08452       0  \n","11               0.18100          0.3792                  0.10480       0  \n","12               0.17670          0.3176                  0.10230       0  \n","13               0.11190          0.2809                  0.06287       0  \n","14               0.22080          0.3596                  0.14310       0  \n","15               0.17120          0.4218                  0.13410       0  \n","16               0.16090          0.3029                  0.08216       0  \n","17               0.20730          0.3706                  0.11420       0  \n","18               0.23880          0.2768                  0.07615       0  \n","19               0.12880          0.2977                  0.07259       1  \n","\n","[20 rows x 31 columns]"],"text/html":["\n","  <div id=\"df-8bd84a71-2be4-4a84-9ab7-b417c12ee636\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>mean radius</th>\n","      <th>mean texture</th>\n","      <th>mean perimeter</th>\n","      <th>mean area</th>\n","      <th>mean smoothness</th>\n","      <th>mean compactness</th>\n","      <th>mean concavity</th>\n","      <th>mean concave points</th>\n","      <th>mean symmetry</th>\n","      <th>mean fractal dimension</th>\n","      <th>...</th>\n","      <th>worst texture</th>\n","      <th>worst perimeter</th>\n","      <th>worst area</th>\n","      <th>worst smoothness</th>\n","      <th>worst compactness</th>\n","      <th>worst concavity</th>\n","      <th>worst concave points</th>\n","      <th>worst symmetry</th>\n","      <th>worst fractal dimension</th>\n","      <th>target</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>17.99</td>\n","      <td>10.38</td>\n","      <td>122.80</td>\n","      <td>1001.0</td>\n","      <td>0.11840</td>\n","      <td>0.27760</td>\n","      <td>0.30010</td>\n","      <td>0.14710</td>\n","      <td>0.2419</td>\n","      <td>0.07871</td>\n","      <td>...</td>\n","      <td>17.33</td>\n","      <td>184.60</td>\n","      <td>2019.0</td>\n","      <td>0.1622</td>\n","      <td>0.6656</td>\n","      <td>0.7119</td>\n","      <td>0.26540</td>\n","      <td>0.4601</td>\n","      <td>0.11890</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>20.57</td>\n","      <td>17.77</td>\n","      <td>132.90</td>\n","      <td>1326.0</td>\n","      <td>0.08474</td>\n","      <td>0.07864</td>\n","      <td>0.08690</td>\n","      <td>0.07017</td>\n","      <td>0.1812</td>\n","      <td>0.05667</td>\n","      <td>...</td>\n","      <td>23.41</td>\n","      <td>158.80</td>\n","      <td>1956.0</td>\n","      <td>0.1238</td>\n","      <td>0.1866</td>\n","      <td>0.2416</td>\n","      <td>0.18600</td>\n","      <td>0.2750</td>\n","      <td>0.08902</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>19.69</td>\n","      <td>21.25</td>\n","      <td>130.00</td>\n","      <td>1203.0</td>\n","      <td>0.10960</td>\n","      <td>0.15990</td>\n","      <td>0.19740</td>\n","      <td>0.12790</td>\n","      <td>0.2069</td>\n","      <td>0.05999</td>\n","      <td>...</td>\n","      <td>25.53</td>\n","      <td>152.50</td>\n","      <td>1709.0</td>\n","      <td>0.1444</td>\n","      <td>0.4245</td>\n","      <td>0.4504</td>\n","      <td>0.24300</td>\n","      <td>0.3613</td>\n","      <td>0.08758</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>11.42</td>\n","      <td>20.38</td>\n","      <td>77.58</td>\n","      <td>386.1</td>\n","      <td>0.14250</td>\n","      <td>0.28390</td>\n","      <td>0.24140</td>\n","      <td>0.10520</td>\n","      <td>0.2597</td>\n","      <td>0.09744</td>\n","      <td>...</td>\n","      <td>26.50</td>\n","      <td>98.87</td>\n","      <td>567.7</td>\n","      <td>0.2098</td>\n","      <td>0.8663</td>\n","      <td>0.6869</td>\n","      <td>0.25750</td>\n","      <td>0.6638</td>\n","      <td>0.17300</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>20.29</td>\n","      <td>14.34</td>\n","      <td>135.10</td>\n","      <td>1297.0</td>\n","      <td>0.10030</td>\n","      <td>0.13280</td>\n","      <td>0.19800</td>\n","      <td>0.10430</td>\n","      <td>0.1809</td>\n","      <td>0.05883</td>\n","      <td>...</td>\n","      <td>16.67</td>\n","      <td>152.20</td>\n","      <td>1575.0</td>\n","      <td>0.1374</td>\n","      <td>0.2050</td>\n","      <td>0.4000</td>\n","      <td>0.16250</td>\n","      <td>0.2364</td>\n","      <td>0.07678</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>12.45</td>\n","      <td>15.70</td>\n","      <td>82.57</td>\n","      <td>477.1</td>\n","      <td>0.12780</td>\n","      <td>0.17000</td>\n","      <td>0.15780</td>\n","      <td>0.08089</td>\n","      <td>0.2087</td>\n","      <td>0.07613</td>\n","      <td>...</td>\n","      <td>23.75</td>\n","      <td>103.40</td>\n","      <td>741.6</td>\n","      <td>0.1791</td>\n","      <td>0.5249</td>\n","      <td>0.5355</td>\n","      <td>0.17410</td>\n","      <td>0.3985</td>\n","      <td>0.12440</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>18.25</td>\n","      <td>19.98</td>\n","      <td>119.60</td>\n","      <td>1040.0</td>\n","      <td>0.09463</td>\n","      <td>0.10900</td>\n","      <td>0.11270</td>\n","      <td>0.07400</td>\n","      <td>0.1794</td>\n","      <td>0.05742</td>\n","      <td>...</td>\n","      <td>27.66</td>\n","      <td>153.20</td>\n","      <td>1606.0</td>\n","      <td>0.1442</td>\n","      <td>0.2576</td>\n","      <td>0.3784</td>\n","      <td>0.19320</td>\n","      <td>0.3063</td>\n","      <td>0.08368</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>13.71</td>\n","      <td>20.83</td>\n","      <td>90.20</td>\n","      <td>577.9</td>\n","      <td>0.11890</td>\n","      <td>0.16450</td>\n","      <td>0.09366</td>\n","      <td>0.05985</td>\n","      <td>0.2196</td>\n","      <td>0.07451</td>\n","      <td>...</td>\n","      <td>28.14</td>\n","      <td>110.60</td>\n","      <td>897.0</td>\n","      <td>0.1654</td>\n","      <td>0.3682</td>\n","      <td>0.2678</td>\n","      <td>0.15560</td>\n","      <td>0.3196</td>\n","      <td>0.11510</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>13.00</td>\n","      <td>21.82</td>\n","      <td>87.50</td>\n","      <td>519.8</td>\n","      <td>0.12730</td>\n","      <td>0.19320</td>\n","      <td>0.18590</td>\n","      <td>0.09353</td>\n","      <td>0.2350</td>\n","      <td>0.07389</td>\n","      <td>...</td>\n","      <td>30.73</td>\n","      <td>106.20</td>\n","      <td>739.3</td>\n","      <td>0.1703</td>\n","      <td>0.5401</td>\n","      <td>0.5390</td>\n","      <td>0.20600</td>\n","      <td>0.4378</td>\n","      <td>0.10720</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>12.46</td>\n","      <td>24.04</td>\n","      <td>83.97</td>\n","      <td>475.9</td>\n","      <td>0.11860</td>\n","      <td>0.23960</td>\n","      <td>0.22730</td>\n","      <td>0.08543</td>\n","      <td>0.2030</td>\n","      <td>0.08243</td>\n","      <td>...</td>\n","      <td>40.68</td>\n","      <td>97.65</td>\n","      <td>711.4</td>\n","      <td>0.1853</td>\n","      <td>1.0580</td>\n","      <td>1.1050</td>\n","      <td>0.22100</td>\n","      <td>0.4366</td>\n","      <td>0.20750</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>16.02</td>\n","      <td>23.24</td>\n","      <td>102.70</td>\n","      <td>797.8</td>\n","      <td>0.08206</td>\n","      <td>0.06669</td>\n","      <td>0.03299</td>\n","      <td>0.03323</td>\n","      <td>0.1528</td>\n","      <td>0.05697</td>\n","      <td>...</td>\n","      <td>33.88</td>\n","      <td>123.80</td>\n","      <td>1150.0</td>\n","      <td>0.1181</td>\n","      <td>0.1551</td>\n","      <td>0.1459</td>\n","      <td>0.09975</td>\n","      <td>0.2948</td>\n","      <td>0.08452</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>11</th>\n","      <td>15.78</td>\n","      <td>17.89</td>\n","      <td>103.60</td>\n","      <td>781.0</td>\n","      <td>0.09710</td>\n","      <td>0.12920</td>\n","      <td>0.09954</td>\n","      <td>0.06606</td>\n","      <td>0.1842</td>\n","      <td>0.06082</td>\n","      <td>...</td>\n","      <td>27.28</td>\n","      <td>136.50</td>\n","      <td>1299.0</td>\n","      <td>0.1396</td>\n","      <td>0.5609</td>\n","      <td>0.3965</td>\n","      <td>0.18100</td>\n","      <td>0.3792</td>\n","      <td>0.10480</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>12</th>\n","      <td>19.17</td>\n","      <td>24.80</td>\n","      <td>132.40</td>\n","      <td>1123.0</td>\n","      <td>0.09740</td>\n","      <td>0.24580</td>\n","      <td>0.20650</td>\n","      <td>0.11180</td>\n","      <td>0.2397</td>\n","      <td>0.07800</td>\n","      <td>...</td>\n","      <td>29.94</td>\n","      <td>151.70</td>\n","      <td>1332.0</td>\n","      <td>0.1037</td>\n","      <td>0.3903</td>\n","      <td>0.3639</td>\n","      <td>0.17670</td>\n","      <td>0.3176</td>\n","      <td>0.10230</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>13</th>\n","      <td>15.85</td>\n","      <td>23.95</td>\n","      <td>103.70</td>\n","      <td>782.7</td>\n","      <td>0.08401</td>\n","      <td>0.10020</td>\n","      <td>0.09938</td>\n","      <td>0.05364</td>\n","      <td>0.1847</td>\n","      <td>0.05338</td>\n","      <td>...</td>\n","      <td>27.66</td>\n","      <td>112.00</td>\n","      <td>876.5</td>\n","      <td>0.1131</td>\n","      <td>0.1924</td>\n","      <td>0.2322</td>\n","      <td>0.11190</td>\n","      <td>0.2809</td>\n","      <td>0.06287</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>14</th>\n","      <td>13.73</td>\n","      <td>22.61</td>\n","      <td>93.60</td>\n","      <td>578.3</td>\n","      <td>0.11310</td>\n","      <td>0.22930</td>\n","      <td>0.21280</td>\n","      <td>0.08025</td>\n","      <td>0.2069</td>\n","      <td>0.07682</td>\n","      <td>...</td>\n","      <td>32.01</td>\n","      <td>108.80</td>\n","      <td>697.7</td>\n","      <td>0.1651</td>\n","      <td>0.7725</td>\n","      <td>0.6943</td>\n","      <td>0.22080</td>\n","      <td>0.3596</td>\n","      <td>0.14310</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>15</th>\n","      <td>14.54</td>\n","      <td>27.54</td>\n","      <td>96.73</td>\n","      <td>658.8</td>\n","      <td>0.11390</td>\n","      <td>0.15950</td>\n","      <td>0.16390</td>\n","      <td>0.07364</td>\n","      <td>0.2303</td>\n","      <td>0.07077</td>\n","      <td>...</td>\n","      <td>37.13</td>\n","      <td>124.10</td>\n","      <td>943.2</td>\n","      <td>0.1678</td>\n","      <td>0.6577</td>\n","      <td>0.7026</td>\n","      <td>0.17120</td>\n","      <td>0.4218</td>\n","      <td>0.13410</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>16</th>\n","      <td>14.68</td>\n","      <td>20.13</td>\n","      <td>94.74</td>\n","      <td>684.5</td>\n","      <td>0.09867</td>\n","      <td>0.07200</td>\n","      <td>0.07395</td>\n","      <td>0.05259</td>\n","      <td>0.1586</td>\n","      <td>0.05922</td>\n","      <td>...</td>\n","      <td>30.88</td>\n","      <td>123.40</td>\n","      <td>1138.0</td>\n","      <td>0.1464</td>\n","      <td>0.1871</td>\n","      <td>0.2914</td>\n","      <td>0.16090</td>\n","      <td>0.3029</td>\n","      <td>0.08216</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>17</th>\n","      <td>16.13</td>\n","      <td>20.68</td>\n","      <td>108.10</td>\n","      <td>798.8</td>\n","      <td>0.11700</td>\n","      <td>0.20220</td>\n","      <td>0.17220</td>\n","      <td>0.10280</td>\n","      <td>0.2164</td>\n","      <td>0.07356</td>\n","      <td>...</td>\n","      <td>31.48</td>\n","      <td>136.80</td>\n","      <td>1315.0</td>\n","      <td>0.1789</td>\n","      <td>0.4233</td>\n","      <td>0.4784</td>\n","      <td>0.20730</td>\n","      <td>0.3706</td>\n","      <td>0.11420</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>18</th>\n","      <td>19.81</td>\n","      <td>22.15</td>\n","      <td>130.00</td>\n","      <td>1260.0</td>\n","      <td>0.09831</td>\n","      <td>0.10270</td>\n","      <td>0.14790</td>\n","      <td>0.09498</td>\n","      <td>0.1582</td>\n","      <td>0.05395</td>\n","      <td>...</td>\n","      <td>30.88</td>\n","      <td>186.80</td>\n","      <td>2398.0</td>\n","      <td>0.1512</td>\n","      <td>0.3150</td>\n","      <td>0.5372</td>\n","      <td>0.23880</td>\n","      <td>0.2768</td>\n","      <td>0.07615</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>19</th>\n","      <td>13.54</td>\n","      <td>14.36</td>\n","      <td>87.46</td>\n","      <td>566.3</td>\n","      <td>0.09779</td>\n","      <td>0.08129</td>\n","      <td>0.06664</td>\n","      <td>0.04781</td>\n","      <td>0.1885</td>\n","      <td>0.05766</td>\n","      <td>...</td>\n","      <td>19.26</td>\n","      <td>99.70</td>\n","      <td>711.2</td>\n","      <td>0.1440</td>\n","      <td>0.1773</td>\n","      <td>0.2390</td>\n","      <td>0.12880</td>\n","      <td>0.2977</td>\n","      <td>0.07259</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>20 rows × 31 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8bd84a71-2be4-4a84-9ab7-b417c12ee636')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-8bd84a71-2be4-4a84-9ab7-b417c12ee636 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-8bd84a71-2be4-4a84-9ab7-b417c12ee636');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":2}]},{"cell_type":"code","source":["# the target is a 0 (malignant) or 1 (benign).\n","cancer_df.target.value_counts()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3rAYJ-ojmnXx","outputId":"3b0d962f-84ba-4057-f0d3-64b81eab2c5c","executionInfo":{"status":"ok","timestamp":1670119248096,"user_tz":360,"elapsed":204,"user":{"displayName":"Matt Massey","userId":"08637701140305391198"}}},"execution_count":3,"outputs":[{"output_type":"execute_result","data":{"text/plain":["1    357\n","0    212\n","Name: target, dtype: int64"]},"metadata":{},"execution_count":3}]},{"cell_type":"markdown","source":["# Holdout Set Example\n","\n","The code below splits the data set 50/50 into training and testing data."],"metadata":{"id":"bO3A04PLBLNn"}},{"cell_type":"code","source":["from sklearn.model_selection import train_test_split\n","from sklearn.naive_bayes import GaussianNB\n","from sklearn.metrics import accuracy_score\n","\n","# Split 50/50 for train and test, set random_state to an int for a static cut of the data\n","data_train, data_test = train_test_split(cancer_df, test_size=.5, random_state=54321)\n","\n","# Separate training/test data into feature matrix and target\n","X_train = data_train.drop(columns='target')\n","X_test = data_test.drop(columns='target')\n","y_train = data_train.target\n","y_test = data_test.target\n","\n","# Create a Gaussian Naive-Bayes model and train it with our 50% holdout\n","gaussiannb = GaussianNB()\n","model = gaussiannb.fit(X_train, y_train)\n","\n","# Generate predictions using the test data\n","y_pred = gaussiannb.predict(X_test)\n","\n","# Print/compare predictions\n","print(y_pred)\n","print(accuracy_score(y_test, y_pred))\n"],"metadata":{"id":"ngFbO177nHVA","colab":{"base_uri":"https://localhost:8080/"},"outputId":"8beb1aeb-1a92-4dbd-d195-447bed87f5b8","executionInfo":{"status":"ok","timestamp":1670119365354,"user_tz":360,"elapsed":213,"user":{"displayName":"Matt Massey","userId":"08637701140305391198"}}},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["[1 0 0 1 1 0 1 0 0 1 1 0 1 1 1 0 0 0 0 0 0 1 1 0 1 0 0 1 0 1 0 1 1 0 1 0 0\n"," 1 1 1 1 0 1 0 1 0 1 1 1 1 0 1 1 0 1 1 0 0 1 0 1 1 1 1 0 1 1 1 1 1 0 1 1 0\n"," 0 1 1 1 1 0 0 0 0 1 1 1 1 1 1 0 0 0 1 0 1 1 1 0 1 1 0 1 0 1 0 1 1 0 1 1 0\n"," 1 0 1 1 1 1 1 1 0 1 1 0 1 1 0 1 0 0 1 1 1 1 1 0 1 1 0 1 0 1 1 0 1 1 1 0 1\n"," 1 1 1 0 1 0 0 0 1 1 1 1 1 1 1 1 0 0 0 0 0 1 0 1 1 1 0 1 0 1 0 0 1 1 1 1 1\n"," 1 0 0 0 0 1 1 1 1 1 0 1 1 1 0 1 1 0 1 1 1 0 1 1 0 1 1 1 1 1 0 0 1 1 1 0 1\n"," 0 1 0 1 1 0 1 1 0 1 1 1 0 1 1 0 1 1 1 0 1 0 1 1 1 1 0 1 1 1 1 1 1 1 1 1 0\n"," 1 0 1 1 0 1 1 1 1 0 1 0 1 1 1 0 1 0 1 1 1 1 1 1 1 0]\n","0.9298245614035088\n"]}]},{"cell_type":"markdown","metadata":{"id":"UNSIV2pP6dJL"},"source":["# Part 0 -- Submission Details\n","\n","\n","(10 points) Please enter your name and the date below. Submit your answers as a completed notebook by the deadline posted on Canvas.  Late submissions will not get credit for this section.\n","\n","Name: ***Matt Massey***\n","\n","Date: ***12/6/2022***\n"]},{"cell_type":"markdown","source":["#Part 1 -- Cross Validation\n","\n","(15 points) Repeat the GaussianNB example, but use 5-fold cross validation. (Hint: *cv=5* when using *cross_val_score*). Print the mean accuracy score and standard deviation.  You may combine parts 1-3 into a single block of code if you wish to use iteration."],"metadata":{"id":"bLRJ2O0VDcoa"}},{"cell_type":"code","source":["# import cross validation module\n","from sklearn.model_selection import cross_val_score\n","\n","# create feature matrix with all data\n","X = cancer_df.drop('target', axis=1)\n","\n","# create target vector of all data\n","y = cancer_df['target']\n","\n","# PARTS 1-3\n","# list of cross validation folds to use for parts 1-3\n","cv_folds = [5,10,20]\n","\n","# loop three cross validation folds (5,10.20) of gaussian naive-bayes classifier\n","for cvf in cv_folds:  \n","  score = cross_val_score(GaussianNB(), X, y, cv=cvf)     # cross-validation score of model\n","  print('Gaussian Naive Bayes, {}-fold cross validation:'.format(cvf))    # print title of model\n","  print('  {0:.2f} mean accuracy'.format(score.mean()))   # print mean accuracy of model\n","  print('  {0:.2f} standarad deviation'.format(score.std()))    # print standard deviation of model accuracy scores\n","  print('\\n')   # print new line before next loop"],"metadata":{"id":"HwpNIxvwDUEv","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1670126045737,"user_tz":360,"elapsed":445,"user":{"displayName":"Matt Massey","userId":"08637701140305391198"}},"outputId":"822c6c4c-f23c-48a7-dbba-f5729aaee236"},"execution_count":78,"outputs":[{"output_type":"stream","name":"stdout","text":["Gaussian Naive Bayes, 5-fold cross validation:\n","  0.94 mean accuracy\n","  0.01 standarad deviation\n","\n","\n","Gaussian Naive Bayes, 10-fold cross validation:\n","  0.94 mean accuracy\n","  0.03 standarad deviation\n","\n","\n","Gaussian Naive Bayes, 20-fold cross validation:\n","  0.94 mean accuracy\n","  0.04 standarad deviation\n","\n","\n"]}]},{"cell_type":"markdown","source":["#Part 2 -- Cross Validation\n","\n","(10 points) Repeat the GaussianNB example, but use 10-fold cross validation. Print the mean accuracy score and standard deviation."],"metadata":{"id":"ioWN4zGyFv8R"}},{"cell_type":"code","source":["# see Part 1 above"],"metadata":{"id":"fJInql8MFsR2","executionInfo":{"status":"ok","timestamp":1670120345229,"user_tz":360,"elapsed":203,"user":{"displayName":"Matt Massey","userId":"08637701140305391198"}}},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":["#Part 3 -- Cross Validation\n","\n","(10 points) Repeat the GaussianNB example, but use 20-fold cross validation. Print the mean accuracy score and standard deviation."],"metadata":{"id":"H8SfVal0GERc"}},{"cell_type":"code","source":["# see Part 1 above"],"metadata":{"id":"8VcmNNiZGDHs","executionInfo":{"status":"ok","timestamp":1670120343819,"user_tz":360,"elapsed":3,"user":{"displayName":"Matt Massey","userId":"08637701140305391198"}}},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":["#Part 4 -- Critical Thinking\n","\n","(15 points) Answer the following questions (as code comments below):\n","\n","1.   Which validation scheme, 50/50 holdout or cross-validation, reported better accuracy? Why?\n","2.   Do your cross validation results appear stable when varying how many folds? What does this suggest about your results?\n","\n"],"metadata":{"id":"bsukIAm3G6A_"}},{"cell_type":"code","source":["# 4.1 --> The cross-validation models all reported better mean accuracy scores than \n","# the holdout set because the cross-validation models are going through the entire \n","# dataset, rather than ony 50% of the dataset.\n","\n","\n","# 4.2 --> The mean accuracy scores for each cross-validation model of 5, 10, and \n","# 20 folds are all 0.94, however, the standard deviation increases with additional \n","# folds. This suggests that there are small local variations in the dataset that\n","# are creating more variance as the validation sets get smaller with the increasing \n","# number of folds, although the overlall mean remains the same. "],"metadata":{"id":"JL7mSzbIG5LV","executionInfo":{"status":"ok","timestamp":1670127375742,"user_tz":360,"elapsed":191,"user":{"displayName":"Matt Massey","userId":"08637701140305391198"}}},"execution_count":89,"outputs":[]},{"cell_type":"markdown","source":["# Part 5 -- Model Swap\n","\n","(15 points) Repeat your 10-fold cross validation, but swap in a model different than GaussianNB. Choose a model appropriate for a classification task.\n","\n","Print the mean accuracy score and standard deviation. Leave as a comment which model performed better and speculate why there may or may not exist a performance difference.\n","\n","Hint: Become familiar with and browse the scikit-learn user guide: https://scikit-learn.org/stable/user_guide.html The \"appropriate\" model is one suitable for this type of task and this type of data."],"metadata":{"id":"wgDmWiNQMJ-3"}},{"cell_type":"code","source":["# import k-neighbors classifier\n","from sklearn.neighbors import KNeighborsClassifier\n","\n","# use the k-neighbors classifier model with all default parameters; neighbors=5\n","kn5 = KNeighborsClassifier()\n","\n","# print model title, mean accuracy, and standard of means for 10-fold cross validation\n","print('k-nearest neighbors (5 neighbors), 10-fold cross validation:')\n","print('  {0:.2f} mean accuracy'.format(cross_val_score(kn5, X, y, cv=10).mean()))\n","print('  {0:.2f} standard deviation'.format(cross_val_score(kn5, X, y, cv=10).std()))"],"metadata":{"id":"CmqU7iHmtdLy","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1670126564016,"user_tz":360,"elapsed":438,"user":{"displayName":"Matt Massey","userId":"08637701140305391198"}},"outputId":"ed0a912f-0240-4000-9e43-191b375de840"},"execution_count":87,"outputs":[{"output_type":"stream","name":"stdout","text":["k-nearest neighbors (5 neighbors), 10-fold cross validation:\n","  0.93 mean accuracy\n","  0.03 standard deviation\n"]}]},{"cell_type":"code","source":["# The k-neighnors model (0.93 +/-0.03) doesn't perform quite as well as the gaussian naive Bayes\n","# model (0.94 +/-0.3), although they are veru close. The k-nearest neighbors uses the default 5 nearest\n","# neighbors and uniform weighting to classify the point. Since the targets are not evenly distributed\n","# in the dataset, the k-nearest neighbors may not have uniform training with the different folds."],"metadata":{"id":"Fo3YivD8HAgu","executionInfo":{"status":"ok","timestamp":1670126162123,"user_tz":360,"elapsed":221,"user":{"displayName":"Matt Massey","userId":"08637701140305391198"}}},"execution_count":80,"outputs":[]},{"cell_type":"markdown","source":["# Part 6 -- Model Swap\n","\n","(15 points) Repeat your 10-fold cross validation, but swap in a model different than GaussianNB (and different than part 5). Choose a model appropriate for a classification task.\n","\n","Print the mean accuracy score and standard deviation. Leave as a comment which model performed better and speculate why there may or may not exist a performance difference."],"metadata":{"id":"v14NscBQP0Zm"}},{"cell_type":"code","source":["# import make_pipeline function, standardscaler to scale dataset for svc classification;\n","# and support vector machine classifier\n","from sklearn.pipeline import make_pipeline\n","from sklearn.preprocessing import StandardScaler\n","from sklearn import svm\n","\n","# support vector classifier standardizes features in cancer dataset (X), then uses the \n","# svm model by using the make_pipeline function; default parameters for model\n","svc = make_pipeline(StandardScaler(), svm.SVC())\n","\n","# print results of model; print title, mean accuracy, and standard deviation of means\n","# for 10 fold cross validation of support vector machine\n","print('C-Support Vector Classification, 10-fold cross validation:')\n","print('  {0:.2f} mean accuracy'.format(cross_val_score(svc, X, y, cv=10).mean()))\n","print('  {0:.2f} standard deviation'.format(cross_val_score(svc, X, y, cv=10).std()))"],"metadata":{"id":"Z8Or8LsfMJf5","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1670126432904,"user_tz":360,"elapsed":488,"user":{"displayName":"Matt Massey","userId":"08637701140305391198"}},"outputId":"7bfa0e04-61fb-4201-d864-00c97aca67f8"},"execution_count":86,"outputs":[{"output_type":"stream","name":"stdout","text":["C-Support Vector Classification, 10-fold cross validation:\n","  0.98 mean accuracy\n","  0.03 standard deviation\n"]}]},{"cell_type":"code","source":["# The support vector machine model has a much better accuracy compared to the gaussian\n","# naive Bayes or k-neighbors models above, with an accuracy of 0.98 +/-0.03. The features\n","# of the cancer dataset are scaled, then used in the model, where the svm fits a \n","# hyperplane (in all dimensions of dataset features) that separates the data into two\n","# classes. This seems like a much more realistic model than assuming a Gaussian distribution\n","# and/or using nearest neighbors."],"metadata":{"id":"rirA1BetUnJV","executionInfo":{"status":"ok","timestamp":1670127147813,"user_tz":360,"elapsed":190,"user":{"displayName":"Matt Massey","userId":"08637701140305391198"}}},"execution_count":88,"outputs":[]},{"cell_type":"markdown","source":["# Part 7 -- Documentation and Correctness\n","(10 points) Please document your code with human-readable messages explaining what the code is doing; at a minimum, every function and control structure should be documented.  If your response is a 1-liner, explain how it works.\n","\n","Additionally, please error check your code; partial credit will be given to answers that do not fully address the requirements. For example, if it says write a function, please make sure your code provides a function.\n","\n","Please make sure your submission has everything completed."],"metadata":{"id":"xaOrlyvYMzLW"}}]}